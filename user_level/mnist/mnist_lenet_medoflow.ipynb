{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with LeNet-5\n",
    "\n",
    "This file is extracted from mnist_dlsys.py, with extra code about logging and plotting added."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "import numpy as np\n",
    "import argparse\n",
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tvm\n",
    "from system_level import autodiff as ad\n",
    "from system_level import tvm_op\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt = \"llvm\"\n",
    "tgt_host = \"llvm\"\n",
    "\n",
    "# create context object\n",
    "executor_ctx = tvm.device(tgt, 0)\n",
    "\n",
    "print_loss_val_each_epoch = True\n",
    "num_epochs = 5\n",
    "\n",
    "def convert_to_one_hot(vals):\n",
    "    \"\"\"Helper method to convert label array to one-hot array.\"\"\"\n",
    "    one_hot_vals = np.zeros((vals.size, 10))\n",
    "    one_hot_vals[np.arange(vals.size), vals] = 1\n",
    "    return one_hot_vals\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded train_set, valid_set and test_set.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the dataset\n",
    "Code adapted from http://deeplearning.net/tutorial/code/logistic_sgd.py\n",
    "\n",
    ":type dataset: string\n",
    ":param dataset: the path to the dataset (here MNIST)\n",
    "\"\"\"\n",
    "# Download the MNIST dataset if it is not present\n",
    "dataset = \"mnist.pkl.gz\"\n",
    "\n",
    "data_dir, data_file = os.path.split(dataset)\n",
    "if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "    # Check if dataset is in the data directory.\n",
    "    new_path = os.path.join(\n",
    "        os.path.split(__file__)[0],\n",
    "        dataset\n",
    "    )\n",
    "    if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "        dataset = new_path\n",
    "\n",
    "if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "    from six.moves import urllib\n",
    "    origin = (\n",
    "        'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "    )\n",
    "    print('Downloading data from %s' % origin)\n",
    "    urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open(dataset, 'rb') as f:\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    except:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "# train_set, valid_set, test_set format: tuple(input, target)\n",
    "# input is a numpy.ndarray of 2 dimensions (a matrix), np.float32\n",
    "# where each row corresponds to an example. target is a\n",
    "# numpy.ndarray of 1 dimension (vector), np.int64 that has the same length\n",
    "# as the number of rows in the input. It should give the target\n",
    "# to the example with the same index in the input.\n",
    "print('Loaded train_set, valid_set and test_set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 784)\n",
      "(50000,)\n",
      "(50000, 1, 28, 28)\n",
      "(50000,)\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape)\n",
    "print(train_set[1].shape)\n",
    "\n",
    "# because we use lenet, we need to reshape the data\n",
    "train_set = (train_set[0].reshape((-1, 1, 28, 28)), train_set[1])\n",
    "valid_set = (valid_set[0].reshape((-1, 1, 28, 28)), valid_set[1])\n",
    "test_set = (test_set[0].reshape((-1, 1, 28, 28)), test_set[1])\n",
    "\n",
    "print(train_set[0].shape)\n",
    "print(train_set[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot a random sample\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(train_set[0][0][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct the computational graph\n",
    "\n",
    "![](../images/lenet.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Declaring Weights ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Declaring Weights ===\")\n",
    "\n",
    "F1 = ad.Variable(name=\"F1\") # conv2d kernel\n",
    "BN1_gamma = ad.Variable(name=\"BN1_gamma\") # batch norm\n",
    "BN1_beta = ad.Variable(name=\"BN1_beta\") # batch norm\n",
    "\n",
    "F2 = ad.Variable(name=\"F2\") # conv2d kernel\n",
    "BN2_gamma = ad.Variable(name=\"BN2_gamma\") # batch norm\n",
    "BN2_beta = ad.Variable(name=\"BN2_beta\") # batch norm\n",
    "\n",
    "W1 = ad.Variable(name=\"W1\") # fully connected weight\n",
    "b1 = ad.Variable(name=\"b1\") # fully connected bias\n",
    "\n",
    "W2 = ad.Variable(name=\"W2\") # fully connected weight\n",
    "b2 = ad.Variable(name=\"b2\") # fully connected bias\n",
    "\n",
    "W3 = ad.Variable(name=\"W3\") # fully connected weight\n",
    "b3 = ad.Variable(name=\"b3\") # fully connected bias\n",
    "\n",
    "X = ad.Variable(name=\"X\") # input\n",
    "y_ = ad.Variable(name=\"y_\") # label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Constructing Computational Graph ===\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Constructing Computational Graph ===\")\n",
    "\n",
    "# CNN1: bn(maxpool(relu(conv(X F))))\n",
    "z1 = ad.conv2d_op(X, F1)\n",
    "z2 = ad.relu_op(z1)\n",
    "z3 = ad.maxpool2d_op(z2, pool_size=2, stride=2)\n",
    "z4 = ad.batchnorm2d_op(z3, BN1_gamma, BN1_beta)\n",
    "\n",
    "# CNN2: bn(maxpool(relu(conv(z3, F2))))\n",
    "z5 = ad.conv2d_op(z4, F2)\n",
    "z6 = ad.relu_op(z5)\n",
    "z7 = ad.maxpool2d_op(z6, pool_size=2, stride=2)\n",
    "z8 = ad.batchnorm2d_op(z7, BN2_gamma, BN2_beta)\n",
    "\n",
    "# flatten\n",
    "CHW = (16, 4, 4)\n",
    "z9 = ad.flatten_op(z8, CHW)\n",
    "\n",
    "# mlp: relu(matmul(z7, W1) + b1)\n",
    "z10 = ad.matmul_op(z9, W1)\n",
    "z11 = ad.relu_op(z10 + ad.broadcastto_op(b1, z10))\n",
    "z12 = ad.matmul_op(z11, W2) \n",
    "z13 = ad.relu_op(z12 + ad.broadcastto_op(b2, z12))\n",
    "z14 = ad.matmul_op(z13, W3)\n",
    "y = z14 + ad.broadcastto_op(b3, z14)\n",
    "\n",
    "# softmax & cross entropy\n",
    "loss = ad.softmaxcrossentropy_op(y, y_)\n",
    "\n",
    "grad_F1, grad_BN1_gamma, grad_BN1_beta, grad_F2, grad_BN2_gamma, grad_BN2_beta, grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3 = ad.gradients(\n",
    "    loss, [F1, BN1_gamma, BN1_beta, F2, BN2_gamma, BN2_beta, W1, W2, W3, b1, b2, b3])\n",
    "     \n",
    "executor = ad.Executor(\n",
    "    [loss, grad_F1, grad_BN1_gamma, grad_BN1_beta, grad_F2, grad_BN2_gamma, grad_BN2_beta, grad_W1, grad_W2, grad_W3, grad_b1, grad_b2, grad_b3, y],\n",
    "    ctx=executor_ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Initializing Weights ===\n",
      "Start training loop...\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Initializing Weights ===\")\n",
    "\n",
    "# Read input data\n",
    "train_set_x, train_set_y = train_set\n",
    "valid_set_x, valid_set_y = valid_set\n",
    "test_set_x, test_set_y = test_set\n",
    "# Set up minibatch\n",
    "batch_size = 1000\n",
    "n_train_batches = train_set_x.shape[0] // batch_size\n",
    "n_valid_batches = valid_set_x.shape[0] // batch_size\n",
    "\n",
    "print(\"Start training loop...\")\n",
    "\n",
    "# Initialize parameters\n",
    "rand = np.random.RandomState(seed=42)\n",
    "# n*1*28*28\n",
    "F1_val = rand.normal(scale=0.1, size=(6, 1, 5, 5)).astype(np.float32)\n",
    "# n*6*24*24\n",
    "# maxpool\n",
    "# n*6*12*12\n",
    "BN1_gamma_val = np.ones(shape=(1, 6, 1, 1)).astype(np.float32)\n",
    "BN1_beta_val = np.zeros(shape=(1, 6, 1, 1)).astype(np.float32)\n",
    "\n",
    "F2_val = rand.normal(scale=0.1, size=(16, 6, 5, 5)).astype(np.float32)\n",
    "# n*16*8*8\n",
    "# maxpool\n",
    "# n*16*4*4\n",
    "BN2_gamma_val = np.ones(shape=(1, 16, 1, 1)).astype(np.float32)\n",
    "BN2_beta_val = np.zeros(shape=(1, 16, 1, 1)).astype(np.float32)\n",
    "\n",
    "W1_val = rand.normal(scale=0.1, size=(16*4*4, 120)).astype(np.float32)\n",
    "b1_val = rand.normal(scale=0.1, size=(120)).astype(np.float32)\n",
    "\n",
    "W2_val = rand.normal(scale=0.1, size=(120, 84)).astype(np.float32)\n",
    "b2_val = rand.normal(scale=0.1, size=(84)).astype(np.float32)\n",
    "\n",
    "W3_val = rand.normal(scale=0.1, size=(84, 10)).astype(np.float32)\n",
    "b3_val = rand.normal(scale=0.1, size=(10)).astype(np.float32)\n",
    "\n",
    "X_val = np.empty(shape=(batch_size, 1, 28, 28), dtype=np.float32)\n",
    "y_val = np.empty(shape=(batch_size, 10), dtype=np.float32)\n",
    "\n",
    "valid_X_val = np.empty(shape=(batch_size, 1, 28, 28), dtype=np.float32)\n",
    "valid_y_val = np.empty(shape=(batch_size, 10), dtype=np.float32)\n",
    "\n",
    "test_X_val = np.empty(shape=(1, 1, 28, 28), dtype=np.float32)\n",
    "test_y_val = np.empty(shape=(1, 10), dtype=np.float32)\n",
    "\n",
    "# wrap with tvm.nd.array\n",
    "F1_val = tvm.nd.array(F1_val)\n",
    "BN1_gamma_val = tvm.nd.array(BN1_gamma_val)\n",
    "BN1_beta_val = tvm.nd.array(BN1_beta_val)\n",
    "F2_val = tvm.nd.array(F2_val)\n",
    "BN2_gamma_val = tvm.nd.array(BN2_gamma_val)\n",
    "BN2_beta_val = tvm.nd.array(BN2_beta_val)\n",
    "W1_val = tvm.nd.array(W1_val)\n",
    "b1_val = tvm.nd.array(b1_val)\n",
    "W2_val = tvm.nd.array(W2_val)\n",
    "b2_val = tvm.nd.array(b2_val)\n",
    "W3_val = tvm.nd.array(W3_val)\n",
    "b3_val = tvm.nd.array(b3_val)\n",
    "\n",
    "X_val = tvm.nd.array(X_val)\n",
    "y_val = tvm.nd.array(y_val)\n",
    "valid_X_val = tvm.nd.array(valid_X_val)\n",
    "valid_y_val = tvm.nd.array(valid_y_val)\n",
    "test_X_val = tvm.nd.array(test_X_val)\n",
    "test_y_val = tvm.nd.array(test_y_val)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train & Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop\n",
    "lr = 1.0e-3\n",
    "# JIT compile sgd update ops\n",
    "F1_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    F1_val.shape, lr, tgt, tgt_host, \"F1_sgd_update\")\n",
    "BN1_gamma_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    BN1_gamma_val.shape, lr, tgt, tgt_host, \"BN1_gamma_sgd_update\")\n",
    "BN1_beta_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    BN1_beta_val.shape, lr, tgt, tgt_host, \"BN1_beta_sgd_update\")\n",
    "\n",
    "F2_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    F2_val.shape, lr, tgt, tgt_host, \"F2_sgd_update\")\n",
    "BN2_gamma_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    BN2_gamma_val.shape, lr, tgt, tgt_host, \"BN2_gamma_sgd_update\")\n",
    "BN2_beta_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    BN2_beta_val.shape, lr, tgt, tgt_host, \"BN2_beta_sgd_update\")\n",
    "\n",
    "W1_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    W1_val.shape, lr, tgt, tgt_host, \"W1_sgd_update\")\n",
    "W2_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    W2_val.shape, lr, tgt, tgt_host, \"W2_sgd_update\")\n",
    "W3_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    W3_val.shape, lr, tgt, tgt_host, \"W3_sgd_update\")\n",
    "b1_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    b1_val.shape, lr, tgt, tgt_host, \"b1_sgd_update\")\n",
    "b2_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    b2_val.shape, lr, tgt, tgt_host, \"b2_sgd_update\")\n",
    "b3_sgd_update_func = tvm_op.make_sgd_update(\n",
    "    b3_val.shape, lr, tgt, tgt_host, \"b3_sgd_update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** EPOCH 1 *****\n",
      "valid minibatch:  0 / 10\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [256000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [30720], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [120000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 256], []), B_1: B_3: Buffer(B_2, float32, [256, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 120], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 30) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*480) + (j.outer*4)) + j.inner.init), 120, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 64) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*480) + cse_var_2) + j.inner)\n",
      "          compute[ramp(cse_var_1, 120, 4)] = (compute[ramp(cse_var_1, 120, 4)] + (A[ramp(((i.outer*1024) + (k.outer*4)), 256, 4)]*broadcast(B[(((k.outer*480) + cse_var_2) + j.inner)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (j.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*480) + cse_var_4) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 120, 4)] = (compute[ramp(cse_var_3, 120, 4)] + (A[ramp((((i.outer*1024) + (k.outer*4)) + 1), 256, 4)]*broadcast(B[((((k.outer*480) + cse_var_4) + j.inner_1) + 120)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (j.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*480) + cse_var_6) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 120, 4)] = (compute[ramp(cse_var_5, 120, 4)] + (A[ramp((((i.outer*1024) + (k.outer*4)) + 2), 256, 4)]*broadcast(B[((((k.outer*480) + cse_var_6) + j.inner_2) + 240)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (j.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*480) + cse_var_8) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 120, 4)] = (compute[ramp(cse_var_7, 120, 4)] + (A[ramp((((i.outer*1024) + (k.outer*4)) + 3), 256, 4)]*broadcast(B[((((k.outer*480) + cse_var_8) + j.inner_3) + 360)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10080], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [84000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 120], []), B_1: B_3: Buffer(B_2, float32, [120, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 84], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 21) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*336) + (j.outer*4)) + j.inner.init), 84, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 30) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*336) + cse_var_2) + j.inner)\n",
      "          compute[ramp(cse_var_1, 84, 4)] = (compute[ramp(cse_var_1, 84, 4)] + (A[ramp(((i.outer*480) + (k.outer*4)), 120, 4)]*broadcast(B[(((k.outer*336) + cse_var_2) + j.inner)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (j.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*336) + cse_var_4) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 84, 4)] = (compute[ramp(cse_var_3, 84, 4)] + (A[ramp((((i.outer*480) + (k.outer*4)) + 1), 120, 4)]*broadcast(B[((((k.outer*336) + cse_var_4) + j.inner_1) + 84)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (j.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*336) + cse_var_6) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 84, 4)] = (compute[ramp(cse_var_5, 84, 4)] + (A[ramp((((i.outer*480) + (k.outer*4)) + 2), 120, 4)]*broadcast(B[((((k.outer*336) + cse_var_6) + j.inner_2) + 168)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (j.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*336) + cse_var_8) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 84, 4)] = (compute[ramp(cse_var_7, 84, 4)] + (A[ramp((((i.outer*480) + (k.outer*4)) + 3), 120, 4)]*broadcast(B[((((k.outer*336) + cse_var_8) + j.inner_3) + 252)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [840], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [10000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 84], []), B_1: B_3: Buffer(B_2, float32, [84, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 10], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 3) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner.init, 2)) < 5), dtype=bool) {\n",
      "          compute[ramp((((i.outer*40) + (j.outer*4)) + j.inner.init), 10, 4)] = broadcast(0f32, 4)\n",
      "        }\n",
      "      }\n",
      "      for (k.outer: int32, 0, 21) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_2: int32 = (j.outer*4)\n",
      "            let cse_var_1: int32 = (((i.outer*40) + cse_var_2) + j.inner)\n",
      "            compute[ramp(cse_var_1, 10, 4)] = (compute[ramp(cse_var_1, 10, 4)] + (A[ramp(((i.outer*336) + (k.outer*4)), 84, 4)]*broadcast(B[(((k.outer*40) + cse_var_2) + j.inner)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_1, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_4: int32 = (j.outer*4)\n",
      "            let cse_var_3: int32 = (((i.outer*40) + cse_var_4) + j.inner_1)\n",
      "            compute[ramp(cse_var_3, 10, 4)] = (compute[ramp(cse_var_3, 10, 4)] + (A[ramp((((i.outer*336) + (k.outer*4)) + 1), 84, 4)]*broadcast(B[((((k.outer*40) + cse_var_4) + j.inner_1) + 10)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_2, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_6: int32 = (j.outer*4)\n",
      "            let cse_var_5: int32 = (((i.outer*40) + cse_var_6) + j.inner_2)\n",
      "            compute[ramp(cse_var_5, 10, 4)] = (compute[ramp(cse_var_5, 10, 4)] + (A[ramp((((i.outer*336) + (k.outer*4)) + 2), 84, 4)]*broadcast(B[((((k.outer*40) + cse_var_6) + j.inner_2) + 20)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_3, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_8: int32 = (j.outer*4)\n",
      "            let cse_var_7: int32 = (((i.outer*40) + cse_var_8) + j.inner_3)\n",
      "            compute[ramp(cse_var_7, 10, 4)] = (compute[ramp(cse_var_7, 10, 4)] + (A[ramp((((i.outer*336) + (k.outer*4)) + 3), 84, 4)]*broadcast(B[((((k.outer*40) + cse_var_8) + j.inner_3) + 30)], 4)))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [10000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [840], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [84000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 10], []), B_1: B_3: Buffer(B_2, float32, [84, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 84], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 21) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*336) + (j.outer*4)) + j.inner.init), 84, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 3) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (k.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*336) + (j.outer*4)) + j.inner)\n",
      "          compute[ramp(cse_var_1, 84, 4)] = (compute[ramp(cse_var_1, 84, 4)] + (A[ramp(((i.outer*40) + cse_var_2), 10, 4)]*broadcast(B[(((j.outer*40) + (j.inner*10)) + cse_var_2)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (k.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*336) + (j.outer*4)) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 84, 4)] = (compute[ramp(cse_var_3, 84, 4)] + (A[ramp((((i.outer*40) + cse_var_4) + 1), 10, 4)]*broadcast(B[((((j.outer*40) + (j.inner_1*10)) + cse_var_4) + 1)], 4)))\n",
      "        }\n",
      "        if @tir.likely((k.outer < 2), dtype=bool) {\n",
      "          for (j.inner_2: int32, 0, 4) {\n",
      "            let cse_var_6: int32 = (k.outer*4)\n",
      "            let cse_var_5: int32 = (((i.outer*336) + (j.outer*4)) + j.inner_2)\n",
      "            compute[ramp(cse_var_5, 84, 4)] = (compute[ramp(cse_var_5, 84, 4)] + (A[ramp((((i.outer*40) + cse_var_6) + 2), 10, 4)]*broadcast(B[((((j.outer*40) + (j.inner_2*10)) + cse_var_6) + 2)], 4)))\n",
      "          }\n",
      "        }\n",
      "        if @tir.likely((k.outer < 2), dtype=bool) {\n",
      "          for (j.inner_3: int32, 0, 4) {\n",
      "            let cse_var_8: int32 = (k.outer*4)\n",
      "            let cse_var_7: int32 = (((i.outer*336) + (j.outer*4)) + j.inner_3)\n",
      "            compute[ramp(cse_var_7, 84, 4)] = (compute[ramp(cse_var_7, 84, 4)] + (A[ramp((((i.outer*40) + cse_var_8) + 3), 10, 4)]*broadcast(B[((((j.outer*40) + (j.inner_3*10)) + cse_var_8) + 3)], 4)))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10080], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [120000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 84], []), B_1: B_3: Buffer(B_2, float32, [120, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 120], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 30) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*480) + (j.outer*4)) + j.inner.init), 120, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 21) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (k.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*480) + (j.outer*4)) + j.inner)\n",
      "          compute[ramp(cse_var_1, 120, 4)] = (compute[ramp(cse_var_1, 120, 4)] + (A[ramp(((i.outer*336) + cse_var_2), 84, 4)]*broadcast(B[(((j.outer*336) + (j.inner*84)) + cse_var_2)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (k.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*480) + (j.outer*4)) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 120, 4)] = (compute[ramp(cse_var_3, 120, 4)] + (A[ramp((((i.outer*336) + cse_var_4) + 1), 84, 4)]*broadcast(B[((((j.outer*336) + (j.inner_1*84)) + cse_var_4) + 1)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (k.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*480) + (j.outer*4)) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 120, 4)] = (compute[ramp(cse_var_5, 120, 4)] + (A[ramp((((i.outer*336) + cse_var_6) + 2), 84, 4)]*broadcast(B[((((j.outer*336) + (j.inner_2*84)) + cse_var_6) + 2)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (k.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*480) + (j.outer*4)) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 120, 4)] = (compute[ramp(cse_var_7, 120, 4)] + (A[ramp((((i.outer*336) + cse_var_8) + 3), 84, 4)]*broadcast(B[((((j.outer*336) + (j.inner_3*84)) + cse_var_8) + 3)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [30720], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [256000], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 120], []), B_1: B_3: Buffer(B_2, float32, [256, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [1000, 256], [])} {\n",
      "  for (i.outer: int32, 0, 250) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 64) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*1024) + (j.outer*4)) + j.inner.init), 256, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 30) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (k.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*1024) + (j.outer*4)) + j.inner)\n",
      "          compute[ramp(cse_var_1, 256, 4)] = (compute[ramp(cse_var_1, 256, 4)] + (A[ramp(((i.outer*480) + cse_var_2), 120, 4)]*broadcast(B[(((j.outer*480) + (j.inner*120)) + cse_var_2)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (k.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*1024) + (j.outer*4)) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 256, 4)] = (compute[ramp(cse_var_3, 256, 4)] + (A[ramp((((i.outer*480) + cse_var_4) + 1), 120, 4)]*broadcast(B[((((j.outer*480) + (j.inner_1*120)) + cse_var_4) + 1)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (k.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*1024) + (j.outer*4)) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 256, 4)] = (compute[ramp(cse_var_5, 256, 4)] + (A[ramp((((i.outer*480) + cse_var_6) + 2), 120, 4)]*broadcast(B[((((j.outer*480) + (j.inner_2*120)) + cse_var_6) + 2)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (k.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*1024) + (j.outer*4)) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 256, 4)] = (compute[ramp(cse_var_7, 256, 4)] + (A[ramp((((i.outer*480) + cse_var_8) + 3), 120, 4)]*broadcast(B[((((j.outer*480) + (j.inner_3*120)) + cse_var_8) + 3)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [256000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [120000], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [30720], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 256], []), B_1: B_3: Buffer(B_2, float32, [1000, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [256, 120], [])} {\n",
      "  for (i.outer: int32, 0, 64) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 30) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*480) + (j.outer*4)) + j.inner.init), 120, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 250) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*480) + cse_var_2) + j.inner)\n",
      "          compute[ramp(cse_var_1, 120, 4)] = (compute[ramp(cse_var_1, 120, 4)] + (A[ramp(((k.outer*1024) + (i.outer*4)), 1, 4)]*broadcast(B[(((k.outer*480) + cse_var_2) + j.inner)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (j.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*480) + cse_var_4) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 120, 4)] = (compute[ramp(cse_var_3, 120, 4)] + (A[ramp((((k.outer*1024) + (i.outer*4)) + 256), 1, 4)]*broadcast(B[((((k.outer*480) + cse_var_4) + j.inner_1) + 120)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (j.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*480) + cse_var_6) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 120, 4)] = (compute[ramp(cse_var_5, 120, 4)] + (A[ramp((((k.outer*1024) + (i.outer*4)) + 512), 1, 4)]*broadcast(B[((((k.outer*480) + cse_var_6) + j.inner_2) + 240)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (j.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*480) + cse_var_8) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 120, 4)] = (compute[ramp(cse_var_7, 120, 4)] + (A[ramp((((k.outer*1024) + (i.outer*4)) + 768), 1, 4)]*broadcast(B[((((k.outer*480) + cse_var_8) + j.inner_3) + 360)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [84000], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [10080], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 120], []), B_1: B_3: Buffer(B_2, float32, [1000, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [120, 84], [])} {\n",
      "  for (i.outer: int32, 0, 30) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 21) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*336) + (j.outer*4)) + j.inner.init), 84, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (k.outer: int32, 0, 250) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*336) + cse_var_2) + j.inner)\n",
      "          compute[ramp(cse_var_1, 84, 4)] = (compute[ramp(cse_var_1, 84, 4)] + (A[ramp(((k.outer*480) + (i.outer*4)), 1, 4)]*broadcast(B[(((k.outer*336) + cse_var_2) + j.inner)], 4)))\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          let cse_var_4: int32 = (j.outer*4)\n",
      "          let cse_var_3: int32 = (((i.outer*336) + cse_var_4) + j.inner_1)\n",
      "          compute[ramp(cse_var_3, 84, 4)] = (compute[ramp(cse_var_3, 84, 4)] + (A[ramp((((k.outer*480) + (i.outer*4)) + 120), 1, 4)]*broadcast(B[((((k.outer*336) + cse_var_4) + j.inner_1) + 84)], 4)))\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (j.outer*4)\n",
      "          let cse_var_5: int32 = (((i.outer*336) + cse_var_6) + j.inner_2)\n",
      "          compute[ramp(cse_var_5, 84, 4)] = (compute[ramp(cse_var_5, 84, 4)] + (A[ramp((((k.outer*480) + (i.outer*4)) + 240), 1, 4)]*broadcast(B[((((k.outer*336) + cse_var_6) + j.inner_2) + 168)], 4)))\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (j.outer*4)\n",
      "          let cse_var_7: int32 = (((i.outer*336) + cse_var_8) + j.inner_3)\n",
      "          compute[ramp(cse_var_7, 84, 4)] = (compute[ramp(cse_var_7, 84, 4)] + (A[ramp((((k.outer*480) + (i.outer*4)) + 360), 1, 4)]*broadcast(B[((((k.outer*336) + cse_var_8) + j.inner_3) + 252)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84000], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10000], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [840], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1000, 84], []), B_1: B_3: Buffer(B_2, float32, [1000, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [84, 10], [])} {\n",
      "  for (i.outer: int32, 0, 21) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 3) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner.init, 2)) < 5), dtype=bool) {\n",
      "          compute[ramp((((i.outer*40) + (j.outer*4)) + j.inner.init), 10, 4)] = broadcast(0f32, 4)\n",
      "        }\n",
      "      }\n",
      "      for (k.outer: int32, 0, 250) {\n",
      "        for (j.inner: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_2: int32 = (j.outer*4)\n",
      "            let cse_var_1: int32 = (((i.outer*40) + cse_var_2) + j.inner)\n",
      "            compute[ramp(cse_var_1, 10, 4)] = (compute[ramp(cse_var_1, 10, 4)] + (A[ramp(((k.outer*336) + (i.outer*4)), 1, 4)]*broadcast(B[(((k.outer*40) + cse_var_2) + j.inner)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_1: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_1, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_4: int32 = (j.outer*4)\n",
      "            let cse_var_3: int32 = (((i.outer*40) + cse_var_4) + j.inner_1)\n",
      "            compute[ramp(cse_var_3, 10, 4)] = (compute[ramp(cse_var_3, 10, 4)] + (A[ramp((((k.outer*336) + (i.outer*4)) + 84), 1, 4)]*broadcast(B[((((k.outer*40) + cse_var_4) + j.inner_1) + 10)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_2, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_6: int32 = (j.outer*4)\n",
      "            let cse_var_5: int32 = (((i.outer*40) + cse_var_6) + j.inner_2)\n",
      "            compute[ramp(cse_var_5, 10, 4)] = (compute[ramp(cse_var_5, 10, 4)] + (A[ramp((((k.outer*336) + (i.outer*4)) + 168), 1, 4)]*broadcast(B[((((k.outer*40) + cse_var_6) + j.inner_2) + 20)], 4)))\n",
      "          }\n",
      "        }\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          if @tir.likely((((j.outer*2) + floordiv(j.inner_3, 2)) < 5), dtype=bool) {\n",
      "            let cse_var_8: int32 = (j.outer*4)\n",
      "            let cse_var_7: int32 = (((i.outer*40) + cse_var_8) + j.inner_3)\n",
      "            compute[ramp(cse_var_7, 10, 4)] = (compute[ramp(cse_var_7, 10, 4)] + (A[ramp((((k.outer*336) + (i.outer*4)) + 252), 1, 4)]*broadcast(B[((((k.outer*40) + cse_var_8) + j.inner_3) + 30)], 4)))\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "valid minibatch:  1 / 10\n",
      "valid minibatch:  2 / 10\n",
      "valid minibatch:  3 / 10\n",
      "valid minibatch:  4 / 10\n",
      "valid minibatch:  5 / 10\n",
      "valid minibatch:  6 / 10\n",
      "valid minibatch:  7 / 10\n",
      "valid minibatch:  8 / 10\n",
      "valid minibatch:  9 / 10\n",
      "accuracy on valid set:  0.0861\n",
      "train minibatch:  0 / 50\n",
      "train minibatch:  1 / 50\n",
      "train minibatch:  2 / 50\n",
      "train minibatch:  3 / 50\n",
      "train minibatch:  4 / 50\n",
      "train minibatch:  5 / 50\n",
      "train minibatch:  6 / 50\n",
      "train minibatch:  7 / 50\n",
      "train minibatch:  8 / 50\n",
      "train minibatch:  9 / 50\n",
      "train minibatch:  10 / 50\n",
      "train minibatch:  11 / 50\n",
      "train minibatch:  12 / 50\n",
      "train minibatch:  13 / 50\n",
      "train minibatch:  14 / 50\n",
      "train minibatch:  15 / 50\n",
      "train minibatch:  16 / 50\n",
      "train minibatch:  17 / 50\n",
      "train minibatch:  18 / 50\n",
      "train minibatch:  19 / 50\n",
      "train minibatch:  20 / 50\n",
      "train minibatch:  21 / 50\n",
      "train minibatch:  22 / 50\n",
      "train minibatch:  23 / 50\n",
      "train minibatch:  24 / 50\n",
      "train minibatch:  25 / 50\n",
      "train minibatch:  26 / 50\n",
      "train minibatch:  27 / 50\n",
      "train minibatch:  28 / 50\n",
      "train minibatch:  29 / 50\n",
      "train minibatch:  30 / 50\n",
      "train minibatch:  31 / 50\n",
      "train minibatch:  32 / 50\n",
      "train minibatch:  33 / 50\n",
      "train minibatch:  34 / 50\n",
      "train minibatch:  35 / 50\n",
      "train minibatch:  36 / 50\n",
      "train minibatch:  37 / 50\n",
      "train minibatch:  38 / 50\n",
      "train minibatch:  39 / 50\n",
      "train minibatch:  40 / 50\n",
      "train minibatch:  41 / 50\n",
      "train minibatch:  42 / 50\n",
      "train minibatch:  43 / 50\n",
      "train minibatch:  44 / 50\n",
      "train minibatch:  45 / 50\n",
      "train minibatch:  46 / 50\n",
      "train minibatch:  47 / 50\n",
      "train minibatch:  48 / 50\n",
      "train minibatch:  49 / 50\n",
      "epoch 1 completed. loss = 0.259166; Time taken this epoch = 712.920057 s\n",
      "***** EPOCH 2 *****\n",
      "valid minibatch:  0 / 10\n",
      "valid minibatch:  1 / 10\n",
      "valid minibatch:  2 / 10\n",
      "valid minibatch:  3 / 10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m X_val\u001b[39m.\u001b[39mcopyfrom(valid_set_x[minibatch_start:minibatch_end])\n\u001b[1;32m     33\u001b[0m y_val\u001b[39m.\u001b[39mcopyfrom(\n\u001b[1;32m     34\u001b[0m     convert_to_one_hot(valid_set_y[minibatch_start:minibatch_end]))\n\u001b[0;32m---> 35\u001b[0m _, _, _, _, _, _, _, _, _, _, _, _, _, y_pred \u001b[39m=\u001b[39m executor\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m     36\u001b[0m     feed_dict\u001b[39m=\u001b[39;49m{\n\u001b[1;32m     37\u001b[0m         X: X_val, \n\u001b[1;32m     38\u001b[0m         y_: y_val, \n\u001b[1;32m     39\u001b[0m         F1: F1_val, \n\u001b[1;32m     40\u001b[0m         BN1_gamma: BN1_gamma_val, \n\u001b[1;32m     41\u001b[0m         BN1_beta: BN1_beta_val, \n\u001b[1;32m     42\u001b[0m         F2: F2_val, \n\u001b[1;32m     43\u001b[0m         BN2_gamma: BN2_gamma_val, \n\u001b[1;32m     44\u001b[0m         BN2_beta: BN2_beta_val, \n\u001b[1;32m     45\u001b[0m         W1: W1_val, \n\u001b[1;32m     46\u001b[0m         W2: W2_val, \n\u001b[1;32m     47\u001b[0m         W3: W3_val, \n\u001b[1;32m     48\u001b[0m         b1: b1_val, \n\u001b[1;32m     49\u001b[0m         b2: b2_val, \n\u001b[1;32m     50\u001b[0m         b3: b3_val})\n\u001b[1;32m     51\u001b[0m val_correct \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msum(y_pred\u001b[39m.\u001b[39masnumpy()\u001b[39m.\u001b[39margmax(axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m==\u001b[39m valid_set_y[minibatch_start:minibatch_end])\n\u001b[1;32m     52\u001b[0m val_total \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m minibatch_end \u001b[39m-\u001b[39m minibatch_start\n",
      "File \u001b[0;32m~/Desktop/Courses/Course23SP/MedoFlow/framework/user_level/mnist/../../system_level/autodiff.py:1092\u001b[0m, in \u001b[0;36mExecutor.run\u001b[0;34m(self, feed_dict, convert_to_numpy_ret_vals, return_hidden)\u001b[0m\n\u001b[1;32m   1090\u001b[0m     node_val \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnode_to_arr_map[node]\n\u001b[1;32m   1091\u001b[0m     \u001b[39m# node_val is modified in-place\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m     node\u001b[39m.\u001b[39;49mop\u001b[39m.\u001b[39;49mcompute(\n\u001b[1;32m   1093\u001b[0m         node, input_vals, node_val, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnode_to_compiled_func[node])\n\u001b[1;32m   1094\u001b[0m     node_to_val_map[node] \u001b[39m=\u001b[39m node_val\n\u001b[1;32m   1095\u001b[0m \u001b[39m# Collect node values.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Courses/Course23SP/MedoFlow/framework/user_level/mnist/../../system_level/autodiff.py:366\u001b[0m, in \u001b[0;36mConv2dGradientFOp.compute\u001b[0;34m(self, node, input_vals, output_val, compiled_func)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute\u001b[39m(\u001b[39mself\u001b[39m, node, input_vals, output_val, compiled_func):\n\u001b[0;32m--> 366\u001b[0m     compiled_func(input_vals[\u001b[39m0\u001b[39;49m], input_vals[\u001b[39m1\u001b[39;49m], input_vals[\u001b[39m2\u001b[39;49m], output_val)\n",
      "File \u001b[0;32m~/miniconda3/envs/medoflow/lib/python3.9/site-packages/tvm/runtime/module.py:198\u001b[0m, in \u001b[0;36mModule.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs):\n\u001b[1;32m    197\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_entry:\n\u001b[0;32m--> 198\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_entry(\u001b[39m*\u001b[39;49margs)\n\u001b[1;32m    199\u001b[0m     \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_func(\u001b[39m*\u001b[39margs)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "time_measurements = []\n",
    "train_acc_values = []\n",
    "val_acc_values = []\n",
    "\n",
    "for i in range(num_epochs):\n",
    "    print(f\"***** EPOCH {i+1} *****\")\n",
    "    \n",
    "    # eval on train set\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    for minibatch_index in tqdm(range(n_train_batches)):\n",
    "        minibatch_start = minibatch_index * batch_size\n",
    "        minibatch_end = (minibatch_index + 1) * batch_size\n",
    "        X_val.copyfrom(train_set_x[minibatch_start:minibatch_end])\n",
    "        y_val.copyfrom(\n",
    "            convert_to_one_hot(train_set_y[minibatch_start:minibatch_end]))\n",
    "        _, _, _, _, _, _, _, _, _, _, _, _, y_pred = executor.run(\n",
    "            feed_dict={X: X_val, y_: y_val, F1: F1_val, F2: F2_val, W1: W1_val, W2: W2_val, W3: W3_val, b1: b1_val, b2: b2_val, b3: b3_val})\n",
    "        train_correct += np.sum(y_pred.asnumpy().argmax(axis=1) == train_set_y[minibatch_start:minibatch_end])\n",
    "        train_total += minibatch_end - minibatch_start\n",
    "    train_acc = train_correct / train_total\n",
    "    train_acc_values.append(train_acc)\n",
    "    print(\"accuracy on training set: \", train_acc)\n",
    "    \n",
    "    # eval on valid set\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    for minibatch_index in range(n_valid_batches):\n",
    "        print(\"valid minibatch: \", minibatch_index, \"/\", n_valid_batches)\n",
    "        minibatch_start = minibatch_index * batch_size\n",
    "        minibatch_end = (minibatch_index + 1) * batch_size\n",
    "        X_val.copyfrom(valid_set_x[minibatch_start:minibatch_end])\n",
    "        y_val.copyfrom(\n",
    "            convert_to_one_hot(valid_set_y[minibatch_start:minibatch_end]))\n",
    "        _, _, _, _, _, _, _, _, _, _, _, _, _, y_pred = executor.run(\n",
    "            feed_dict={\n",
    "                X: X_val, \n",
    "                y_: y_val, \n",
    "                F1: F1_val, \n",
    "                BN1_gamma: BN1_gamma_val, \n",
    "                BN1_beta: BN1_beta_val, \n",
    "                F2: F2_val, \n",
    "                BN2_gamma: BN2_gamma_val, \n",
    "                BN2_beta: BN2_beta_val, \n",
    "                W1: W1_val, \n",
    "                W2: W2_val, \n",
    "                W3: W3_val, \n",
    "                b1: b1_val, \n",
    "                b2: b2_val, \n",
    "                b3: b3_val})\n",
    "        val_correct += np.sum(y_pred.asnumpy().argmax(axis=1) == valid_set_y[minibatch_start:minibatch_end])\n",
    "        val_total += minibatch_end - minibatch_start\n",
    "    val_acc = val_correct / val_total\n",
    "    val_acc_values.append(val_acc)\n",
    "    print(\"accuracy on valid set: \", val_acc)\n",
    "    \n",
    "    # train on train set\n",
    "    start_time = time.time()\n",
    "    for minibatch_index in range(n_train_batches):\n",
    "        print(\"train minibatch: \", minibatch_index, \"/\", n_train_batches)\n",
    "        minibatch_start = minibatch_index * batch_size\n",
    "        minibatch_end = (minibatch_index + 1) * batch_size\n",
    "        X_val.copyfrom(train_set_x[minibatch_start:minibatch_end])\n",
    "        y_val.copyfrom(\n",
    "            convert_to_one_hot(train_set_y[minibatch_start:minibatch_end]))\n",
    "        \n",
    "        loss_val, grad_F1_val, grad_BN1_gamma_val, grad_BN1_beta_val, \\\n",
    "            grad_F2_val, grad_BN2_gamma_val, grad_BN2_beta_val, \\\n",
    "            grad_W1_val, grad_W2_val, grad_W3_val, \\\n",
    "            grad_b1_val, grad_b2_val, grad_b3_val, _ = executor.run(\n",
    "                feed_dict={\n",
    "                    X: X_val,\n",
    "                    y_: y_val,\n",
    "                    F1: F1_val,\n",
    "                    BN1_gamma: BN1_gamma_val,\n",
    "                    BN1_beta: BN1_beta_val,\n",
    "                    F2: F2_val,\n",
    "                    BN2_gamma: BN2_gamma_val,\n",
    "                    BN2_beta: BN2_beta_val,\n",
    "                    W1: W1_val,\n",
    "                    W2: W2_val,\n",
    "                    W3: W3_val,\n",
    "                    b1: b1_val,\n",
    "                    b2: b2_val,\n",
    "                    b3: b3_val})\n",
    "        # SGD update\n",
    "        F1_sgd_update_func(F1_val, grad_F1_val, F1_val)\n",
    "        BN1_gamma_sgd_update_func(BN1_gamma_val, grad_BN1_gamma_val, BN1_gamma_val)\n",
    "        BN1_beta_sgd_update_func(BN1_beta_val, grad_BN1_beta_val, BN1_beta_val)\n",
    "        F2_sgd_update_func(F2_val, grad_F2_val, F2_val)\n",
    "        BN2_gamma_sgd_update_func(BN2_gamma_val, grad_BN2_gamma_val, BN2_gamma_val)\n",
    "        BN2_beta_sgd_update_func(BN2_beta_val, grad_BN2_beta_val, BN2_beta_val)\n",
    "        W1_sgd_update_func(W1_val, grad_W1_val, W1_val)\n",
    "        W2_sgd_update_func(W2_val, grad_W2_val, W2_val)\n",
    "        W3_sgd_update_func(W3_val, grad_W3_val, W3_val)\n",
    "        b1_sgd_update_func(b1_val, grad_b1_val, b1_val)\n",
    "        b2_sgd_update_func(b2_val, grad_b2_val, b2_val)\n",
    "        b3_sgd_update_func(b3_val, grad_b3_val, b3_val)\n",
    "    \n",
    "    time_measurements.append(time.time() - start_time)\n",
    "    if print_loss_val_each_epoch:\n",
    "        print(\"epoch %d completed.\" % (i+1), \"loss = %f; Time taken this epoch = %f s\" \n",
    "            % (loss_val.asnumpy()[0].astype(np.float64), time_measurements[-1]))\n",
    "        \n",
    "print(\"Average Time per Training Epoch = %f s\" % np.mean(time_measurements))\n",
    "# w/o batchnorm: [0.0832, 0.261, 0.1037, 0.3641, 0.2317, 0.1784]\n",
    "# w/ batchnorm: [0.0861, 0.94, 0.954, 0.9588, 0.9637, 0.9655, 0.9666, 0.9672, 0.968, 0.9694]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0861, 0.94, 0.954, 0.9588, 0.9637, 0.9655, 0.9666, 0.9672, 0.968, 0.9694]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save train_acc_values = [] val_acc_values = [] loss_values = [] to a json file\n",
    "import json\n",
    "with open(\"results/medoflow_mlp.json\" , \"w\") as f:\n",
    "    json.dump({\"train_acc\": train_acc_values, \n",
    "                \"val_acc\": val_acc_values,\n",
    "                \"loss\": loss_values,\n",
    "                \"time\": time_measurements}, f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_acc_values, label='Training accuracy')\n",
    "plt.plot(val_acc_values, label='Validation accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [256], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [30720], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [120], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 256], []), B_1: B_3: Buffer(B_2, float32, [256, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 120], [])} {\n",
      "  for (j.outer: int32, 0, 30) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, 64) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (j.outer*4)\n",
      "        let cse_var_1: int32 = (cse_var_2 + j.inner)\n",
      "        compute[cse_var_1] = (compute[cse_var_1] + (A[(k.outer*4)]*B[(((k.outer*480) + cse_var_2) + j.inner)]))\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        let cse_var_4: int32 = (j.outer*4)\n",
      "        let cse_var_3: int32 = (cse_var_4 + j.inner_1)\n",
      "        compute[cse_var_3] = (compute[cse_var_3] + (A[((k.outer*4) + 1)]*B[((((k.outer*480) + cse_var_4) + j.inner_1) + 120)]))\n",
      "      }\n",
      "      for (j.inner_2: int32, 0, 4) {\n",
      "        let cse_var_6: int32 = (j.outer*4)\n",
      "        let cse_var_5: int32 = (cse_var_6 + j.inner_2)\n",
      "        compute[cse_var_5] = (compute[cse_var_5] + (A[((k.outer*4) + 2)]*B[((((k.outer*480) + cse_var_6) + j.inner_2) + 240)]))\n",
      "      }\n",
      "      for (j.inner_3: int32, 0, 4) {\n",
      "        let cse_var_8: int32 = (j.outer*4)\n",
      "        let cse_var_7: int32 = (cse_var_8 + j.inner_3)\n",
      "        compute[cse_var_7] = (compute[cse_var_7] + (A[((k.outer*4) + 3)]*B[((((k.outer*480) + cse_var_8) + j.inner_3) + 360)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10080], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [84], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 120], []), B_1: B_3: Buffer(B_2, float32, [120, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 84], [])} {\n",
      "  for (j.outer: int32, 0, 21) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, 30) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (j.outer*4)\n",
      "        let cse_var_1: int32 = (cse_var_2 + j.inner)\n",
      "        compute[cse_var_1] = (compute[cse_var_1] + (A[(k.outer*4)]*B[(((k.outer*336) + cse_var_2) + j.inner)]))\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        let cse_var_4: int32 = (j.outer*4)\n",
      "        let cse_var_3: int32 = (cse_var_4 + j.inner_1)\n",
      "        compute[cse_var_3] = (compute[cse_var_3] + (A[((k.outer*4) + 1)]*B[((((k.outer*336) + cse_var_4) + j.inner_1) + 84)]))\n",
      "      }\n",
      "      for (j.inner_2: int32, 0, 4) {\n",
      "        let cse_var_6: int32 = (j.outer*4)\n",
      "        let cse_var_5: int32 = (cse_var_6 + j.inner_2)\n",
      "        compute[cse_var_5] = (compute[cse_var_5] + (A[((k.outer*4) + 2)]*B[((((k.outer*336) + cse_var_6) + j.inner_2) + 168)]))\n",
      "      }\n",
      "      for (j.inner_3: int32, 0, 4) {\n",
      "        let cse_var_8: int32 = (j.outer*4)\n",
      "        let cse_var_7: int32 = (cse_var_8 + j.inner_3)\n",
      "        compute[cse_var_7] = (compute[cse_var_7] + (A[((k.outer*4) + 3)]*B[((((k.outer*336) + cse_var_8) + j.inner_3) + 252)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [840], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [10], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 84], []), B_1: B_3: Buffer(B_2, float32, [84, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 10], [])} {\n",
      "  for (j.outer: int32, 0, 3) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      if @tir.likely((((j.outer*2) + floordiv(j.inner.init, 2)) < 5), dtype=bool) {\n",
      "        compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "      }\n",
      "    }\n",
      "    for (k.outer: int32, 0, 21) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner, 2)) < 5), dtype=bool) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (cse_var_2 + j.inner)\n",
      "          compute[cse_var_1] = (compute[cse_var_1] + (A[(k.outer*4)]*B[(((k.outer*40) + cse_var_2) + j.inner)]))\n",
      "        }\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner_1, 2)) < 5), dtype=bool) {\n",
      "          let cse_var_4: int32 = (j.outer*4)\n",
      "          let cse_var_3: int32 = (cse_var_4 + j.inner_1)\n",
      "          compute[cse_var_3] = (compute[cse_var_3] + (A[((k.outer*4) + 1)]*B[((((k.outer*40) + cse_var_4) + j.inner_1) + 10)]))\n",
      "        }\n",
      "      }\n",
      "      for (j.inner_2: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner_2, 2)) < 5), dtype=bool) {\n",
      "          let cse_var_6: int32 = (j.outer*4)\n",
      "          let cse_var_5: int32 = (cse_var_6 + j.inner_2)\n",
      "          compute[cse_var_5] = (compute[cse_var_5] + (A[((k.outer*4) + 2)]*B[((((k.outer*40) + cse_var_6) + j.inner_2) + 20)]))\n",
      "        }\n",
      "      }\n",
      "      for (j.inner_3: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner_3, 2)) < 5), dtype=bool) {\n",
      "          let cse_var_8: int32 = (j.outer*4)\n",
      "          let cse_var_7: int32 = (cse_var_8 + j.inner_3)\n",
      "          compute[cse_var_7] = (compute[cse_var_7] + (A[((k.outer*4) + 3)]*B[((((k.outer*40) + cse_var_8) + j.inner_3) + 30)]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [10], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [840], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [84], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 10], []), B_1: B_3: Buffer(B_2, float32, [84, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 84], [])} {\n",
      "  for (j.outer: int32, 0, 21) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, 3) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (k.outer*4)\n",
      "        let cse_var_1: int32 = ((j.outer*4) + j.inner)\n",
      "        compute[cse_var_1] = (compute[cse_var_1] + (A[cse_var_2]*B[(((j.outer*40) + (j.inner*10)) + cse_var_2)]))\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        let cse_var_4: int32 = (k.outer*4)\n",
      "        let cse_var_3: int32 = ((j.outer*4) + j.inner_1)\n",
      "        compute[cse_var_3] = (compute[cse_var_3] + (A[(cse_var_4 + 1)]*B[((((j.outer*40) + (j.inner_1*10)) + cse_var_4) + 1)]))\n",
      "      }\n",
      "      if @tir.likely((k.outer < 2), dtype=bool) {\n",
      "        for (j.inner_2: int32, 0, 4) {\n",
      "          let cse_var_6: int32 = (k.outer*4)\n",
      "          let cse_var_5: int32 = ((j.outer*4) + j.inner_2)\n",
      "          compute[cse_var_5] = (compute[cse_var_5] + (A[(cse_var_6 + 2)]*B[((((j.outer*40) + (j.inner_2*10)) + cse_var_6) + 2)]))\n",
      "        }\n",
      "      }\n",
      "      if @tir.likely((k.outer < 2), dtype=bool) {\n",
      "        for (j.inner_3: int32, 0, 4) {\n",
      "          let cse_var_8: int32 = (k.outer*4)\n",
      "          let cse_var_7: int32 = ((j.outer*4) + j.inner_3)\n",
      "          compute[cse_var_7] = (compute[cse_var_7] + (A[(cse_var_8 + 3)]*B[((((j.outer*40) + (j.inner_3*10)) + cse_var_8) + 3)]))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10080], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [120], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 84], []), B_1: B_3: Buffer(B_2, float32, [120, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 120], [])} {\n",
      "  for (j.outer: int32, 0, 30) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, 21) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (k.outer*4)\n",
      "        let cse_var_1: int32 = ((j.outer*4) + j.inner)\n",
      "        compute[cse_var_1] = (compute[cse_var_1] + (A[cse_var_2]*B[(((j.outer*336) + (j.inner*84)) + cse_var_2)]))\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        let cse_var_4: int32 = (k.outer*4)\n",
      "        let cse_var_3: int32 = ((j.outer*4) + j.inner_1)\n",
      "        compute[cse_var_3] = (compute[cse_var_3] + (A[(cse_var_4 + 1)]*B[((((j.outer*336) + (j.inner_1*84)) + cse_var_4) + 1)]))\n",
      "      }\n",
      "      for (j.inner_2: int32, 0, 4) {\n",
      "        let cse_var_6: int32 = (k.outer*4)\n",
      "        let cse_var_5: int32 = ((j.outer*4) + j.inner_2)\n",
      "        compute[cse_var_5] = (compute[cse_var_5] + (A[(cse_var_6 + 2)]*B[((((j.outer*336) + (j.inner_2*84)) + cse_var_6) + 2)]))\n",
      "      }\n",
      "      for (j.inner_3: int32, 0, 4) {\n",
      "        let cse_var_8: int32 = (k.outer*4)\n",
      "        let cse_var_7: int32 = ((j.outer*4) + j.inner_3)\n",
      "        compute[cse_var_7] = (compute[cse_var_7] + (A[(cse_var_8 + 3)]*B[((((j.outer*336) + (j.inner_3*84)) + cse_var_8) + 3)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [30720], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [256], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 120], []), B_1: B_3: Buffer(B_2, float32, [256, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [1, 256], [])} {\n",
      "  for (j.outer: int32, 0, 64) {\n",
      "    for (j.inner.init: int32, 0, 4) {\n",
      "      compute[((j.outer*4) + j.inner.init)] = 0f32\n",
      "    }\n",
      "    for (k.outer: int32, 0, 30) {\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (k.outer*4)\n",
      "        let cse_var_1: int32 = ((j.outer*4) + j.inner)\n",
      "        compute[cse_var_1] = (compute[cse_var_1] + (A[cse_var_2]*B[(((j.outer*480) + (j.inner*120)) + cse_var_2)]))\n",
      "      }\n",
      "      for (j.inner_1: int32, 0, 4) {\n",
      "        let cse_var_4: int32 = (k.outer*4)\n",
      "        let cse_var_3: int32 = ((j.outer*4) + j.inner_1)\n",
      "        compute[cse_var_3] = (compute[cse_var_3] + (A[(cse_var_4 + 1)]*B[((((j.outer*480) + (j.inner_1*120)) + cse_var_4) + 1)]))\n",
      "      }\n",
      "      for (j.inner_2: int32, 0, 4) {\n",
      "        let cse_var_6: int32 = (k.outer*4)\n",
      "        let cse_var_5: int32 = ((j.outer*4) + j.inner_2)\n",
      "        compute[cse_var_5] = (compute[cse_var_5] + (A[(cse_var_6 + 2)]*B[((((j.outer*480) + (j.inner_2*120)) + cse_var_6) + 2)]))\n",
      "      }\n",
      "      for (j.inner_3: int32, 0, 4) {\n",
      "        let cse_var_8: int32 = (k.outer*4)\n",
      "        let cse_var_7: int32 = ((j.outer*4) + j.inner_3)\n",
      "        compute[cse_var_7] = (compute[cse_var_7] + (A[(cse_var_8 + 3)]*B[((((j.outer*480) + (j.inner_3*120)) + cse_var_8) + 3)]))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [256], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [120], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [30720], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 256], []), B_1: B_3: Buffer(B_2, float32, [1, 120], []), compute_1: compute_3: Buffer(compute_2, float32, [256, 120], [])} {\n",
      "  for (i.outer: int32, 0, 64) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 30) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*480) + (j.outer*4)) + j.inner.init), 120, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (j.outer*4)\n",
      "        let cse_var_1: int32 = (((i.outer*480) + cse_var_2) + j.inner)\n",
      "        compute[ramp(cse_var_1, 120, 4)] = (compute[ramp(cse_var_1, 120, 4)] + (A[ramp((i.outer*4), 1, 4)]*broadcast(B[(cse_var_2 + j.inner)], 4)))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [120], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [84], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [10080], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 120], []), B_1: B_3: Buffer(B_2, float32, [1, 84], []), compute_1: compute_3: Buffer(compute_2, float32, [120, 84], [])} {\n",
      "  for (i.outer: int32, 0, 30) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 21) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        compute[ramp((((i.outer*336) + (j.outer*4)) + j.inner.init), 84, 4)] = broadcast(0f32, 4)\n",
      "      }\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        let cse_var_2: int32 = (j.outer*4)\n",
      "        let cse_var_1: int32 = (((i.outer*336) + cse_var_2) + j.inner)\n",
      "        compute[ramp(cse_var_1, 84, 4)] = (compute[ramp(cse_var_1, 84, 4)] + (A[ramp((i.outer*4), 1, 4)]*broadcast(B[(cse_var_2 + j.inner)], 4)))\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "@main = primfn(A_1: handle, B_1: handle, compute_1: handle) -> ()\n",
      "  attr = {\"from_legacy_te_schedule\": True, \"global_symbol\": \"main\", \"tir.noalias\": True}\n",
      "  buffers = {A: Buffer(A_2: Pointer(float32), float32, [84], []),\n",
      "             B: Buffer(B_2: Pointer(float32), float32, [10], []),\n",
      "             compute: Buffer(compute_2: Pointer(float32), float32, [840], [])}\n",
      "  buffer_map = {A_1: A, B_1: B, compute_1: compute}\n",
      "  preflattened_buffer_map = {A_1: A_3: Buffer(A_2, float32, [1, 84], []), B_1: B_3: Buffer(B_2, float32, [1, 10], []), compute_1: compute_3: Buffer(compute_2, float32, [84, 10], [])} {\n",
      "  for (i.outer: int32, 0, 21) \"parallel\" {\n",
      "    for (j.outer: int32, 0, 3) {\n",
      "      for (j.inner.init: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner.init, 2)) < 5), dtype=bool) {\n",
      "          compute[ramp((((i.outer*40) + (j.outer*4)) + j.inner.init), 10, 4)] = broadcast(0f32, 4)\n",
      "        }\n",
      "      }\n",
      "      for (j.inner: int32, 0, 4) {\n",
      "        if @tir.likely((((j.outer*2) + floordiv(j.inner, 2)) < 5), dtype=bool) {\n",
      "          let cse_var_2: int32 = (j.outer*4)\n",
      "          let cse_var_1: int32 = (((i.outer*40) + cse_var_2) + j.inner)\n",
      "          compute[ramp(cse_var_1, 10, 4)] = (compute[ramp(cse_var_1, 10, 4)] + (A[ramp((i.outer*4), 1, 4)]*broadcast(B[(cse_var_2 + j.inner)], 4)))\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "\n",
      "Predicted class = 8\n",
      "Actual class = 8\n"
     ]
    }
   ],
   "source": [
    "# visualize the data\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "\n",
    "# pick a random image from the test set\n",
    "idx = np.random.randint(0, test_set[0].shape[0])\n",
    "img = test_set_x[idx:idx+1]\n",
    "# plot the image\n",
    "plt.imshow(img[0][0], cmap=cm.Greys_r)\n",
    "plt.show()\n",
    "\n",
    "# get predictions\n",
    "test_X_val.copyfrom(img)\n",
    "test_y_val.copyfrom(convert_to_one_hot(test_set_y[idx:idx+1]))\n",
    "_, _, _, _, _, _, _, _, _, _, _, _, _, test_y_predicted = executor.run(\n",
    "    feed_dict={\n",
    "        X: test_X_val,\n",
    "        y_: test_y_val,\n",
    "        F1: F1_val,\n",
    "        BN1_gamma: BN1_gamma_val,\n",
    "        BN1_beta: BN1_beta_val,\n",
    "        F2: F2_val,\n",
    "        BN2_gamma: BN2_gamma_val,\n",
    "        BN2_beta: BN2_beta_val,\n",
    "        W1: W1_val,\n",
    "        W2: W2_val,\n",
    "        W3: W3_val,\n",
    "        b1: b1_val,\n",
    "        b2: b2_val,\n",
    "        b3: b3_val},\n",
    "    convert_to_numpy_ret_vals=True);\n",
    "print(\"Predicted class = %d\" % np.argmax(test_y_predicted))\n",
    "print(\"Actual class = %d\" % test_set_y[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medoflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
