{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:29:23.586014: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import gzip\n",
    "import pickle\n",
    "import time\n",
    "import numpy as np \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and examine dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded train_set, valid_set and test_set.\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load the dataset\n",
    "Code adapted from http://deeplearning.net/tutorial/code/logistic_sgd.py\n",
    "\n",
    ":type dataset: string\n",
    ":param dataset: the path to the dataset (here MNIST)\n",
    "\"\"\"\n",
    "# Download the MNIST dataset if it is not present\n",
    "dataset = \"mnist.pkl.gz\"\n",
    "\n",
    "data_dir, data_file = os.path.split(dataset)\n",
    "if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "    # Check if dataset is in the data directory.\n",
    "    new_path = os.path.join(\n",
    "        os.path.split(__file__)[0],\n",
    "        dataset\n",
    "    )\n",
    "    if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "        dataset = new_path\n",
    "\n",
    "if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "    from six.moves import urllib\n",
    "    origin = (\n",
    "        'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "    )\n",
    "    print('Downloading data from %s' % origin)\n",
    "    urllib.request.urlretrieve(origin, dataset)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "# Load the dataset\n",
    "with gzip.open(dataset, 'rb') as f:\n",
    "    try:\n",
    "        train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "    except:\n",
    "        train_set, valid_set, test_set = pickle.load(f)\n",
    "\n",
    "print('Loaded train_set, valid_set and test_set.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_set = (train_set[0].astype('float32'), train_set[1].astype('int64'))\n",
    "valid_set = (valid_set[0].astype('float32'), valid_set[1].astype('int64'))\n",
    "test_set = (test_set[0].astype('float32'), test_set[1].astype('int64'))\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices(train_set)\n",
    "train_loader = train_dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "valid_dataset = tf.data.Dataset.from_tensor_slices(valid_set)\n",
    "valid_loader = valid_dataset.batch(batch_size)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(test_set)\n",
    "test_loader = test_dataset.batch(batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:29:29.421814: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_0' with dtype float and shape [50000,784]\n",
      "\t [[{{node Placeholder/_0}}]]\n",
      "2023-04-30 14:29:29.422108: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [50000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "780/782 [============================>.] - ETA: 0s - loss: 0.4440 - sparse_categorical_accuracy: 0.8779"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-30 14:29:31.694349: I tensorflow/core/common_runtime/executor.cc:1197] [/device:CPU:0] (DEBUG INFO) Executor start aborting (this does not indicate an error and you can ignore this message): INVALID_ARGUMENT: You must feed a value for placeholder tensor 'Placeholder/_1' with dtype int64 and shape [10000]\n",
      "\t [[{{node Placeholder/_1}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 [==============================] - 3s 3ms/step - loss: 0.4439 - sparse_categorical_accuracy: 0.8779 - val_loss: 0.3309 - val_sparse_categorical_accuracy: 0.8987\n",
      "Epoch 2/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.2151 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.2241 - val_sparse_categorical_accuracy: 0.9338\n",
      "Epoch 3/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1606 - sparse_categorical_accuracy: 0.9534 - val_loss: 0.1390 - val_sparse_categorical_accuracy: 0.9618\n",
      "Epoch 4/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1268 - sparse_categorical_accuracy: 0.9642 - val_loss: 0.1846 - val_sparse_categorical_accuracy: 0.9397\n",
      "Epoch 5/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.1038 - sparse_categorical_accuracy: 0.9701 - val_loss: 0.1145 - val_sparse_categorical_accuracy: 0.9657\n",
      "Epoch 6/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0883 - sparse_categorical_accuracy: 0.9746 - val_loss: 0.1056 - val_sparse_categorical_accuracy: 0.9694\n",
      "Epoch 7/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0748 - sparse_categorical_accuracy: 0.9789 - val_loss: 0.1052 - val_sparse_categorical_accuracy: 0.9701\n",
      "Epoch 8/20\n",
      "782/782 [==============================] - 2s 3ms/step - loss: 0.0648 - sparse_categorical_accuracy: 0.9819 - val_loss: 0.1122 - val_sparse_categorical_accuracy: 0.9687\n",
      "Epoch 9/20\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0570 - sparse_categorical_accuracy: 0.9841 - val_loss: 0.0916 - val_sparse_categorical_accuracy: 0.9727\n",
      "Epoch 10/20\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0502 - sparse_categorical_accuracy: 0.9860 - val_loss: 0.0833 - val_sparse_categorical_accuracy: 0.9749\n",
      "Epoch 11/20\n",
      "782/782 [==============================] - 3s 3ms/step - loss: 0.0437 - sparse_categorical_accuracy: 0.9882 - val_loss: 0.0869 - val_sparse_categorical_accuracy: 0.9756\n",
      "Epoch 12/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0386 - sparse_categorical_accuracy: 0.9897 - val_loss: 0.0808 - val_sparse_categorical_accuracy: 0.9759\n",
      "Epoch 13/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0335 - sparse_categorical_accuracy: 0.9913 - val_loss: 0.0815 - val_sparse_categorical_accuracy: 0.9762\n",
      "Epoch 14/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0301 - sparse_categorical_accuracy: 0.9923 - val_loss: 0.0784 - val_sparse_categorical_accuracy: 0.9770\n",
      "Epoch 15/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0263 - sparse_categorical_accuracy: 0.9934 - val_loss: 0.0766 - val_sparse_categorical_accuracy: 0.9769\n",
      "Epoch 16/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0232 - sparse_categorical_accuracy: 0.9947 - val_loss: 0.0775 - val_sparse_categorical_accuracy: 0.9777\n",
      "Epoch 17/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0203 - sparse_categorical_accuracy: 0.9960 - val_loss: 0.0764 - val_sparse_categorical_accuracy: 0.9772\n",
      "Epoch 18/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0181 - sparse_categorical_accuracy: 0.9966 - val_loss: 0.0770 - val_sparse_categorical_accuracy: 0.9786\n",
      "Epoch 19/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0161 - sparse_categorical_accuracy: 0.9969 - val_loss: 0.0854 - val_sparse_categorical_accuracy: 0.9751\n",
      "Epoch 20/20\n",
      "782/782 [==============================] - 3s 4ms/step - loss: 0.0142 - sparse_categorical_accuracy: 0.9978 - val_loss: 0.0782 - val_sparse_categorical_accuracy: 0.9781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x16b6d2ee0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(100, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.SGD(0.05),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    train_loader,\n",
    "    epochs=20,\n",
    "    validation_data=valid_loader,\n",
    "    # output epoch 0 as well\n",
    "    initial_epoch=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# save the results: train_loss_list, train_acc_list, valid_acc_list\n",
    "train_loss_list = model.history.history['loss']\n",
    "train_acc_list = model.history.history['sparse_categorical_accuracy']\n",
    "valid_acc_list = model.history.history['val_sparse_categorical_accuracy']\n",
    "\n",
    "with open(\"results/tensorflow_mlp.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"train_loss\": train_loss_list,\n",
    "        \"train_acc\": train_acc_list,\n",
    "        \"valid_acc\": valid_acc_list\n",
    "    }, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medoflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
